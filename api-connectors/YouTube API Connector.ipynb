{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1420788",
   "metadata": {},
   "source": [
    "# YouTube API Connector\n",
    "\n",
    "First of all, let's import the libraries to connect to the YouTube API and manipulate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ab8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import googleapiclient.errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f34de",
   "metadata": {},
   "source": [
    "Then, let's add the credentials to connect to the YouTube Data v3 API. This will provide us a generic information about the channel, including the ID of the playlist where all the videos are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "581aaf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 'youtube#channelListResponse', 'etag': 'xm0qCYcUUWM0F3XMlRFDWwej_n8', 'pageInfo': {'totalResults': 1, 'resultsPerPage': 5}, 'items': [{'kind': 'youtube#channel', 'etag': 'eyLQNm6fwyqFzcXjoobAzVEH-Cw', 'id': 'UCzQTrA_c1BgRNLewVyt2UFw', 'contentDetails': {'relatedPlaylists': {'likes': '', 'uploads': 'UUzQTrA_c1BgRNLewVyt2UFw'}}}]}\n"
     ]
    }
   ],
   "source": [
    "# Defining the API Key\n",
    "api_key = 'api_key'\n",
    "\n",
    "# Defining the API name and version before connection\n",
    "youtube_api_service_name = \"youtube\"\n",
    "youtube_api_version = \"v3\"\n",
    "\n",
    "youtube = build(youtube_api_service_name, youtube_api_version, developerKey = api_key)\n",
    "\n",
    "# Gathering the channel content details\n",
    "request = youtube.channels().list(part='ContentDetails', id = 'UCzQTrA_c1BgRNLewVyt2UFw')\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aba4ad",
   "metadata": {},
   "source": [
    "In the next step, this playlist ID will allow to provide us to get all the video IDs, the title, the description, and the publish date. I have created a function to gather all this data for this step.\n",
    "\n",
    "As it is only possible to get the data from only 50 videos, it is necessary to use the 'nextPageToken' to extract the data from the rest of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e75bddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding read permissions to the API\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "\n",
    "# Creating a function to gather all the video information from the previous playlistId\n",
    "def gather_youtube_videos(playlistId):\n",
    "\n",
    "    youtube = build(youtube_api_service_name, youtube_api_version, developerKey = api_key)\n",
    "    res = youtube.playlistItems().list(part=\"snippet\", playlistId='UUzQTrA_c1BgRNLewVyt2UFw', maxResults=\"50\").execute()\n",
    "\n",
    "    nextPageToken = res.get('nextPageToken')\n",
    "    \n",
    "    # Creating a while structure to gather the video from the rest of the pages. \n",
    "    #It will stop when there hare no more 'nextPageToken' available\n",
    "    while ('nextPageToken' in res):\n",
    "        nextPage = youtube.playlistItems().list(\n",
    "        part=\"snippet\",\n",
    "        playlistId=playlistId,\n",
    "        maxResults=\"50\",\n",
    "        pageToken=nextPageToken\n",
    "        ).execute()\n",
    "        \n",
    "        res['items'] = res['items'] + nextPage['items']\n",
    "\n",
    "        if 'nextPageToken' not in nextPage:\n",
    "            res.pop('nextPageToken', None)\n",
    "        else:\n",
    "            nextPageToken = nextPage['nextPageToken']\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e6646c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the PlayListId to gather the basic data from the videos\n",
    "info_videos = gather_youtube_videos('UUzQTrA_c1BgRNLewVyt2UFw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13356af",
   "metadata": {},
   "source": [
    "Let's extract the basic information for each video from the previous dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48164fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the variables to store the data from the dictionary\n",
    "video_id = []\n",
    "title = []\n",
    "description = []\n",
    "publish_date = []\n",
    "video_url = []\n",
    "\n",
    "# Gather all the video IDs from the dictionary\n",
    "for item in info_videos[\"items\"]:\n",
    "    video_id.append(item['snippet']['resourceId']['videoId'])\n",
    "\n",
    "# Gather all the titles from the dictionary\n",
    "for item in info_videos[\"items\"]:\n",
    "    title.append(item['snippet']['title'])\n",
    "\n",
    "# Gather all the descriptions from the dictionary\n",
    "for item in info_videos[\"items\"]:\n",
    "    description.append(item['snippet']['description'])\n",
    "\n",
    "# Gather all the publish dates from the dictionary\n",
    "for item in info_videos[\"items\"]:\n",
    "    publish_date.append(item['snippet']['publishedAt'])\n",
    "      \n",
    "# Create the video URLs with the video IDs\n",
    "for item in range(len(info_videos['items'])):\n",
    "    video_url.append(f'https://youtu.be/{video_id[item]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bd81e",
   "metadata": {},
   "source": [
    "Let's save all those lists in a new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5044858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataframe\n",
    "df = pd.DataFrame({'publish_date' : publish_date, 'video_id' : video_id, 'video_url': video_url, \n",
    "                   'title' : title, 'description' : description})\n",
    "\n",
    "# Modifying the publish_date as datetime object\n",
    "df['publish_date'] = df['publish_date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f49b94",
   "metadata": {},
   "source": [
    "Now, I will create a copy of this DataFrame and add the 'Grade' variable to introduce if it's E.S.O. (from 7th to 10th grade) or Bachillerato (11th & 12th grades)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa20919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the DataFrame\n",
    "df_content = df.copy()\n",
    "\n",
    "# Assining grades to each video\n",
    "df_content['Grade'] = '2 Bachillerato'\n",
    "df_content.loc[df_content['title'].str.contains('ESO'), 'Grade'] = 'ESO/1 Bachillerato'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa2859",
   "metadata": {},
   "source": [
    "In the following step, I will gather different metrics (views, likes, shares...) from all the videos on a daily basis and store it on a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30c158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=852632511285-09e8025vv77tqnorhjgp8lof8j6j67cm.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fyoutube.readonly&state=8vrGKDStuzwEImr0kRluYCGxsZkw4p&prompt=consent&access_type=offline\n",
      "Enter the authorization code: 4/1ARtbsJoCAZoSUFuL383fBGLz0oa9bGQ-QQ7105dJHwfaPt6LoZ06-2ntXEA\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary to store the data\n",
    "dic = {}\n",
    "\n",
    "# Creating a function to gather the data\n",
    "def main():\n",
    "\n",
    "    api_service_name = \"youtubeAnalytics\"\n",
    "    api_version = \"v2\"\n",
    "    client_secrets_file = \"file_name.json\"\n",
    "\n",
    "    # Get credentials and create API client\n",
    "    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n",
    "        client_secrets_file, scopes)\n",
    "    credentials = flow.run_console()\n",
    "    youtube_analytics = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, credentials=credentials)\n",
    "\n",
    "    # Get the metrics for each video\n",
    "    for i in range(len(video_id)):\n",
    "        request1 = youtube_analytics.reports().query(\n",
    "            dimensions=\"day\",\n",
    "            endDate=\"2022-10-10\",\n",
    "            ids=\"channel==MINE\",\n",
    "            maxResults=150,\n",
    "            metrics=\"views,likes,comments,dislikes,shares,subscribersLost,subscribersGained,estimatedMinutesWatched,averageViewDuration,averageViewPercentage,annotationImpressions,annotationClicks,annotationClickThroughRate,cardImpressions,cardClicks,cardClickRate\",\n",
    "            filters = f'video=={video_id[i]}',\n",
    "            startDate=\"2021-03-01\"\n",
    "    )\n",
    "        response2 = request1.execute()\n",
    "        \n",
    "        dic[video_id[i]] = response2['rows']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af63c13",
   "metadata": {},
   "source": [
    "Now, all the data gathered has been stored in a dictionary. Let's transform that dictionary into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "844b5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the variables extracted from each video\n",
    "video_metrics = ['date', 'views','likes','comments','dislikes','shares','subscribersLost','subscribersGained',\n",
    "           'estimatedMinutesWatched','averageViewDuration','averageViewPercentage','annotationImpressions',\n",
    "           'annotationClicks','annotationClickThroughRate','cardImpressions','cardClicks','cardClickRate']\n",
    "\n",
    "# Use the .items() method to store the key and the values separately in the DataFrame\n",
    "df_metrics = pd.DataFrame(dic.items())\n",
    "\n",
    "# Renaming the columns\n",
    "df_metrics.rename(columns={0: 'video_id', 1: 'metrics'}, inplace=True, errors='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4523e7",
   "metadata": {},
   "source": [
    "I will use the function .explode() in the next step to transform each element of a list-like to a row, replicating the index values. That's why I will a reset_index() function at the end of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a49300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DQvmA2RAcmc</td>\n",
       "      <td>[2022-06-28, 2, 0, 0, 0, 0, 0, 0, 0, 19, 2.73,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DQvmA2RAcmc</td>\n",
       "      <td>[2022-06-29, 2, 2, 1, 0, 0, 0, 0, 7, 229, 31.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DQvmA2RAcmc</td>\n",
       "      <td>[2022-06-30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DQvmA2RAcmc</td>\n",
       "      <td>[2022-07-01, 1, 0, 0, 0, 0, 0, 0, 1, 106, 14.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DQvmA2RAcmc</td>\n",
       "      <td>[2022-07-02, 2, 0, 0, 0, 0, 0, 0, 1, 35, 4.89,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10866</th>\n",
       "      <td>oD04M7jB75s</td>\n",
       "      <td>[2021-10-07, 9, 0, 0, 0, 0, 0, 0, 0, 2, 0.89, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10867</th>\n",
       "      <td>oD04M7jB75s</td>\n",
       "      <td>[2021-10-08, 4, 0, 0, 0, 0, 0, 0, 2, 41, 13.45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10868</th>\n",
       "      <td>oD04M7jB75s</td>\n",
       "      <td>[2021-10-09, 8, 0, 0, 0, 0, 0, 0, 4, 31, 10.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10869</th>\n",
       "      <td>oD04M7jB75s</td>\n",
       "      <td>[2021-10-10, 11, 0, 0, 0, 0, 0, 0, 12, 70, 22....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>oD04M7jB75s</td>\n",
       "      <td>[2021-10-11, 14, 1, 0, 0, 0, 0, 1, 22, 94, 30....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10871 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                            metrics\n",
       "0      DQvmA2RAcmc  [2022-06-28, 2, 0, 0, 0, 0, 0, 0, 0, 19, 2.73,...\n",
       "1      DQvmA2RAcmc  [2022-06-29, 2, 2, 1, 0, 0, 0, 0, 7, 229, 31.9...\n",
       "2      DQvmA2RAcmc  [2022-06-30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3      DQvmA2RAcmc  [2022-07-01, 1, 0, 0, 0, 0, 0, 0, 1, 106, 14.8...\n",
       "4      DQvmA2RAcmc  [2022-07-02, 2, 0, 0, 0, 0, 0, 0, 1, 35, 4.89,...\n",
       "...            ...                                                ...\n",
       "10866  oD04M7jB75s  [2021-10-07, 9, 0, 0, 0, 0, 0, 0, 0, 2, 0.89, ...\n",
       "10867  oD04M7jB75s  [2021-10-08, 4, 0, 0, 0, 0, 0, 0, 2, 41, 13.45...\n",
       "10868  oD04M7jB75s  [2021-10-09, 8, 0, 0, 0, 0, 0, 0, 4, 31, 10.19...\n",
       "10869  oD04M7jB75s  [2021-10-10, 11, 0, 0, 0, 0, 0, 0, 12, 70, 22....\n",
       "10870  oD04M7jB75s  [2021-10-11, 14, 1, 0, 0, 0, 0, 1, 22, 94, 30....\n",
       "\n",
       "[10871 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .explode() function to desaggregate the lists on the metrics columns\n",
    "df_stacked = df_metrics.explode('metrics').reset_index(drop=True)\n",
    "df_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0437b02",
   "metadata": {},
   "source": [
    "Now, let's do some final transformations before exporting this DataFrame to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ab42b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xsof\\AppData\\Local\\Temp\\ipykernel_10560\\3231103768.py:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_stacked['metrics'] = df_stacked['metrics'].str.replace('[', '')\n",
      "C:\\Users\\xsof\\AppData\\Local\\Temp\\ipykernel_10560\\3231103768.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_stacked['metrics'] = df_stacked['metrics'].str.replace(']', '')\n"
     ]
    }
   ],
   "source": [
    "# Changing the metrics variable as string to do some replacements\n",
    "df_stacked['metrics'] = df_stacked['metrics'].astype(str)\n",
    "\n",
    "# Replace the brackers by nothing\n",
    "df_stacked['metrics'] = df_stacked['metrics'].str.replace('[', '')\n",
    "df_stacked['metrics'] = df_stacked['metrics'].str.replace(']', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6024ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the metrics columns into several columns\n",
    "df_stacked[video_metrics] = df_stacked['metrics'].str.split(',', expand = True)\n",
    "df_stacked.drop('metrics', axis = 1, inplace = True)\n",
    "df_stacked['date'] = df_stacked['date'].str.replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stacked.to_csv('video_performance.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2e1ec",
   "metadata": {},
   "source": [
    "In the following steps, I will connect to Google BigQuery with two different goals:\n",
    "\n",
    "1. Create a table to store the data recently extracted\n",
    "2. Pull the output file got in the previous step to that Google BigQuery table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries to connect with Google BigQuery\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Adding the credentials from the JSON file to connect with the platform\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "'file_name.json')\n",
    "project_id = 'project_name'\n",
    "client = bigquery.Client(credentials= credentials, project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac869d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set table_id to the ID of the table to create\n",
    "table_id = 'table_name'\n",
    "\n",
    "# Add the schema of the table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"video_id\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"date\", \"DATE\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"views\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"likes\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"comments\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"dislikes\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"shares\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"subscribersLost\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"subscribersGained\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"estimatedMinutesWatched\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"averageViewDuration\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"averageViewPercentage\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"annotationImpressions\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"annotationClicks\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"annotationClickThroughRate\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cardImpressions\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cardClicks\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cardClickRate\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "# Make an API request to create the table\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table)  \n",
    "print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354208b6",
   "metadata": {},
   "source": [
    "Finally, let's push the .csv() file created to Google BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the name of the file\n",
    "file_path = 'video_performance.csv'\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1, autodetect=True,\n",
    ")\n",
    "\n",
    "# Open and read the file\n",
    "with open(file_path, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "job.result() \n",
    "\n",
    "# Make an API request to upload the table\n",
    "table = client.get_table(table_id)  \n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), table_id\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
